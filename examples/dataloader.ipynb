{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accuracy_bounds.data.sentinel2 import SRDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from accuracy_bounds.inverseproblems.utils import torch_csr_to_scipy, torch_sparse_to_scipy_csr\n",
    "from accuracy_bounds.inverseproblems.kersize_compute_dataloader import (target_distances_samplingYX_perbatch_cuda,\n",
    "    kersize_samplingYX,\n",
    "    avgLB_samplingYX,\n",
    "    avgkersize_samplingYX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/localhome/gawl_ja/Dokumente/Kernel/archive\"\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared Datasets\n",
      "    input_data:            1999\n",
      "    target_data1:           2000\n",
      "    target_data:2           2000\n",
      "    forwarded_target_data: 1999\n"
     ]
    }
   ],
   "source": [
    "input_data = SRDataset(folder_path=data_path, suffixes=('lr_res'))\n",
    "target_data1 = SRDataset(folder_path=data_path, suffixes=('hr_res'))\n",
    "target_data2 = SRDataset(folder_path=data_path, suffixes=('hr_res'))\n",
    "forwarded_target_data = SRDataset(folder_path=data_path, suffixes=('lr_res'))\n",
    "\n",
    "print(\"Prepared Datasets\")\n",
    "print(\"    input_data:           \", len(input_data))\n",
    "print(\"    target_data1:          \", len(target_data1))\n",
    "print(\"    target_data:2          \", len(target_data2))\n",
    "print(\"    forwarded_target_data:\", len(forwarded_target_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared DataLoaders\n",
      "    input_loader:            19 batches with size 100\n",
      "    target_loader1:           20 batches with size 100\n",
      "    target_loader2:           20 batches with size 100\n",
      "    forwarded_target_loader: 19 batches with size 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localhome/gawl_ja/micromamba/envs/dl4gam/lib/python3.13/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 100 worker processes in total. Our suggested max number of worker in current system is 22, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_loader = DataLoader(input_data, batch_size=batch_size, num_workers=batch_size, drop_last=True)\n",
    "target_loader1 = DataLoader(target_data1, batch_size=batch_size, num_workers=batch_size, drop_last=True)\n",
    "target_loader2 = DataLoader(target_data2, batch_size=batch_size, num_workers=batch_size, drop_last=True)\n",
    "forwarded_target_loader = DataLoader(forwarded_target_data, batch_size=batch_size, num_workers=batch_size, drop_last=True)\n",
    "\n",
    "print(\"Prepared DataLoaders\")\n",
    "print(\"    input_loader:           \", len(input_loader), \"batches with size\", batch_size)\n",
    "print(\"    target_loader1:          \", len(target_loader1), \"batches with size\", batch_size)\n",
    "print(\"    target_loader2:          \", len(target_loader2), \"batches with size\", batch_size)\n",
    "print(\"    forwarded_target_loader:\", len(forwarded_target_loader), \"batches with size\", batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Batch: [1 / 19],     Input Batch: [1 / 19]                    \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localhome/gawl_ja/Dokumente/Kernel/AccuracyBounds/src/accuracy_bounds/inverseproblems/kersize_compute_dataloader.py:561: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  forwarded_target = torch.tensor(forwarded_target, dtype = torch.float32, device = device)\n",
      "/localhome/gawl_ja/Dokumente/Kernel/AccuracyBounds/src/accuracy_bounds/inverseproblems/kersize_compute_dataloader.py:562: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_data = torch.tensor(input_data, dtype = torch.float32, device = device)\n",
      "/localhome/gawl_ja/Dokumente/Kernel/AccuracyBounds/src/accuracy_bounds/inverseproblems/kersize_compute_dataloader.py:566: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_data_flat = torch.tensor(input_data.reshape(n,-1), device = device)\n",
      "/localhome/gawl_ja/Dokumente/Kernel/AccuracyBounds/src/accuracy_bounds/inverseproblems/kersize_compute_dataloader.py:567: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  forwarded_flat = torch.tensor(forwarded_target.reshape(m,-1),device = device)\n",
      "/localhome/gawl_ja/Dokumente/Kernel/AccuracyBounds/src/accuracy_bounds/inverseproblems/kersize_compute_dataloader.py:436: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1730827426462/work/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  feasible_small = feasible_small.to_sparse_csr()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Batch1: [20 / 20],     Target Batch2: [20 / 20]                    \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m feasible_appartenance = torch_csr_to_scipy(feasible_appartenance)\n\u001b[32m      6\u001b[39m distsXX = torch_sparse_to_scipy_csr(distsXX)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m wc_kersize = \u001b[43mkersize_samplingYX\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistsXX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeasible_appartenance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_X\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m/(N)\n\u001b[32m      8\u001b[39m avg_LB =  avgLB_samplingYX(distsXX, feasible_appartenance, p_X = \u001b[32m1\u001b[39m)/(N)\n\u001b[32m      9\u001b[39m avg_kersize = avgkersize_samplingYX(distsXX, feasible_appartenance, p_X = \u001b[32m1\u001b[39m)/(N)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumente/Kernel/AccuracyBounds/src/accuracy_bounds/inverseproblems/kersize_compute_dataloader.py:606\u001b[39m, in \u001b[36mkersize_samplingYX\u001b[39m\u001b[34m(distsXX, feasible_appartenance, p_X)\u001b[39m\n\u001b[32m    603\u001b[39m n,p = feasible_appartenance.shape\n\u001b[32m    604\u001b[39m \u001b[38;5;66;03m#distsXX = torch.asarray(distsXX.to_dense())\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m results = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_max_distance\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeasible_appartenance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistsXX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.nanmax(np.array(\u001b[38;5;28mlist\u001b[39m(results)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/dl4gam/lib/python3.13/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/dl4gam/lib/python3.13/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/dl4gam/lib/python3.13/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "noise_level = 5000\n",
    "N = 16**2\n",
    "\n",
    "print(\"Compute Feasible Appartenance\")\n",
    "distsXX, feasible_appartenance = target_distances_samplingYX_perbatch_cuda(0, input_loader, target_loader1, target_loader2, forwarded_target_loader, p_X=1, p_Y=2, epsilon = (noise_level*N)**(1/2))\n",
    "\n",
    "print(\"Convert to Scipy Sparse\")\n",
    "feasible_appartenance = torch_csr_to_scipy(feasible_appartenance)\n",
    "distsXX = torch_sparse_to_scipy_csr(distsXX)\n",
    "\n",
    "print(\"Compute WC Kernelsize\")\n",
    "wc_kersize = kersize_samplingYX(distsXX, feasible_appartenance, p_X = 1)/(N)\n",
    "\n",
    "print(\"Compute Avg. Lowerbound\")\n",
    "avg_LB =  avgLB_samplingYX(distsXX, feasible_appartenance, p_X = 1)/(N)\n",
    "\n",
    "print(\"Compute Avg. Kernelsize\")\n",
    "avg_kersize = avgkersize_samplingYX(distsXX, feasible_appartenance, p_X = 1)/(N)\n",
    "\n",
    "print(f'Worst case kernelsize : {wc_kersize} \\n Average lower bound : {avg_LB} \\n Average kersize : {avg_kersize}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
